{
  "hash": "5f6c6f2485c294539de87b6d7456c759",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Quantitative approach to topology generators\nauthors:\n  - name: Tanya Strydom\n    id: ts\n    orcid: 0000-0001-6067-1349\ndate: last-modified\nbibliography: ../references.bib\n---\n\n\nI think in the long run I will probably turn the other repo ([BecksLab/topology_generators](https://github.com/BecksLab/topology_generators)) into a project that we can then access and I will port all of the working code here... I think it will just come down to what the 'cost' is of the computational side of things and how convoluted it might end up looking.\n\nAnyway so for now I am just pulling in the INTERIM data from some of the models. This uses (for now) 20 mangal [@poisotMangalMakingEcological2016] datasets and just generates some networks for using the niche and cascade models. The 'results' for now is just counting the number of links each model generates for each network because that's what my brain decided on doing...\n\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\n#| include: false\n#| warning: false\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n#| include: false\n#| warning: false\n# download file from other repo\ndownload.file(\"https://raw.githubusercontent.com/BecksLab/topology_generators/main/data/topology_summary.csv\", \"../notebooks/data/topology_models.csv\")\n\ndf = read.csv(\"data/topology_models.csv\")  %>% \n        filter(complexity_real != 0) %>% \n        mutate(ratio_real = top_real/basal_real,\n                top_real = NULL,\n                basal_real = NULL,\n                ratio_mod = top_mod/basal_mod,\n                top_mod = NULL,\n                basal_mod = NULL) %>%\n        pivot_longer(\n            cols = c(complexity_real, distance_real, connectance_real, ratio_real), \n            names_to = \"real\",\n            values_to = \"real_val\")  %>% \n        pivot_longer(\n            cols = c(complexity_mod, distance_mod, connectance_mod, ratio_mod), \n            names_to = \"test\",\n            values_to = \"model_val\")  %>% \n        mutate(\n            real = str_extract(real, \"[^_]*\"),\n            test = str_extract(test, \"[^_]*\"))  %>% \n        filter(real == test) %>%\n        ungroup() %>% \n        filter(model_val != Inf)\n```\n:::\n\n\nFrom a data transformation perspective instead of looking at the raw number of 'top' (zero vulnerability) and 'bottom' (zero generality) species we can instead look at the ratio of top:bottom. A small (< 1) number will thus be indicative of a 'bottom-heavy' network and the opposite for larger numbers\n\nLets start by summarising the 'raw' data as box plots just to see what it looks like before we calculate the Z scores\n\n\n::: {#cell-fig-boxplot .cell}\n\n```{.r .cell-code .hidden}\n#| warning: false\n#| echo: false\n#| label: fig-boxplot\n#| fig-cap: \"Boxplot looking at raw values for each measurement for each model. The horizontal line represents the true overall mean for each measurement\"\n\nggplot(df) +\n    geom_boxplot(aes(x = real,\n                    y = model_val,\n                    colour = model)) +\n    geom_hline(data = df  %>% \n                        group_by(real)  %>% \n                        reframe(mu_sim = mean(model_val, na.rm = TRUE)),\n                aes(yintercept = mu_sim),\n                alpha = 0.7) +\n    facet_wrap(vars(real),\n                scales = 'free') +\n    scale_size(guide = 'none') +\n    theme_classic() +\n    theme(panel.border = element_rect(colour = 'black',\n                                      fill = \"#ffffff00\"),\n            axis.text.x = element_blank(),\n            axis.ticks.x = element_blank())\n```\n\n::: {.cell-output-display}\n![Boxplot looking at raw values for each measurement for each model. The horizontal line represents the true overall mean for each measurement](model_quantitative_files/figure-html/fig-boxplot-1.png){#fig-boxplot width=672}\n:::\n:::\n\n\n\nNow we can look at the Z scores for the different models for the different network measures that we use.\n\nHere 'Z score' is calculated as:\n\n$$\n Z = \\frac{x_{real}-\\mu_{model}}{\\sigma_{model}}\n$$\n\n\n::: {#cell-fig-topology .cell}\n\n```{.r .cell-code .hidden}\n#| warning: false\n#| echo: false\n#| label: fig-topology\n#| fig-cap: \"Z-scores for network summary statistics. Negative Z-scores indicate a (mean) value greater than expected. The magnitude of Z-score probably also tells us how 'variable'/constrained the model is... I am aware that there are no Z scores for some of the Random models and am looking into it\"\n\nggplot(df %>% \n        group_by(id, real, model)  %>% \n        reframe(x_real = real_val,\n                mu_sim = mean(model_val, na.rm = TRUE),\n                sd_sim = sd(model_val, na.rm = TRUE)) %>%\n        mutate(z_score = ((x_real-mu_sim)/sd_sim)) %>%\n        distinct()) +\n    geom_vline(aes(xintercept = 0)) +\n    geom_histogram(aes(x = z_score,\n                    fill = model),\n                colour = \"#ffffff00\") +\n    facet_grid(rows = vars(model),\n                cols = vars(real),\n                scales = \"free\") +\n    scale_size(guide = 'none') +\n    coord_cartesian(expand = FALSE) +\n    theme_classic() +\n    theme(panel.border = element_rect(colour = 'black',\n                                      fill = \"#ffffff00\"))\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning: Removed 36 rows containing non-finite values (`stat_bin()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Z-scores for network summary statistics. Negative Z-scores indicate a (mean) value greater than expected. The magnitude of Z-score probably also tells us how 'variable'/constrained the model is... I am aware that there are no Z scores for some of the Random models and am looking into it](model_quantitative_files/figure-html/fig-topology-1.png){#fig-topology width=672}\n:::\n:::\n\n\n## References {.unnumbered}\n\n::: {#refs}\n:::",
    "supporting": [
      "model_quantitative_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}