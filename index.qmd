---
title: T is for Topology
author:
  - name: Tanya Strydom
    orcid: 0000-0001-6067-1349
    corresponding: true
    email: t.strydom@sheffield.ac.uk
    roles:
      - Investigation
      - Project administration
      - Software
      - Visualisation
    affiliations:
      - University of Sheffield
  - name: Andrew P. Beckerman
    orcid: 0000-0002-7859-8394
    corresponding: false
    roles: []
    affiliations:
      - University of Sheffield
keywords:
  - food web
  - network construction
abstract: |
  Pending...
plain-language-summary: |
  We want to know a bit more about the different network topology generators (predict tools) and how they differ - *i.e.,*  their strengths and weaknesses
key-points:
  - Important point 1
  - Important point 2
date: last-modified
bibliography: references.bib
citation:
  container-title: Earth and Space Science
number-sections: true
jupyter: python3
---

## Introduction

The standard run of the mill that we cannot always feasibly construct networks because 1. hard, 2. time (yay dinosaurs, but also the future and impending doom I guess), and 3. probably something else meaningful that's just slipping my mind at the moment.

Maybe a brief history of the development of predictive tools? Sort of where the theory/body of work was based and how that has changed?

Maybe start here with discussing the core mechanistic differences that models will work at --- some are really concerned about (and thus constrained by) structure, others are more mechanistic in nature *i.e.,* species *a* has the capacity to eat species *b* because traits (read gob size), and then you get @rohrModelingFoodWebs2010 and @strydomFoodWebReconstruction2022 that sit in the weird liminal latent space...

At some point we are going to need to discuss the key differences and implications between predicting a metaweb and a network realisation.

> Do we need to delve into individual-based networks? (*sensu* Tinker 2012, Ara√∫jo 2008) I think its probably a step too far and one starts delving into apples and pears type of comparisons. Especially since these work off of already existing networks and its more about about 'tweaking' those - so not so much *de novo* predictions. Although this might be useful to keep in mind when it comes to re-wiring... Also on that note do we opn the re-wiring door here in this ms or wait it out a bit.

## Data & Methods {#sec-data-methods}

### Overview of topology generators

I know table are awful but in this case they may make more sense

| Model             | Reference                                    | Core Mechanism     |
|-------------------|----------------------------------------------|--------------------|
| Niche model       | @cohenStochasticTheoryCommunity1997          | structural         |
| Cascade model     | @williamsSimpleRulesYield2000                | structural         |
| PFIM              | @shawFrameworkReconstructingAncient2024      | mechanistic        |
| Log-ratio         | @rohrModelingFoodWebs2010                    | latent trait space |
| Nested hierarchy  | @cattinPhylogeneticConstraintsAdaptation2004 |                    |
| ADBM              | @petcheySizeForagingFood2008                 | mechanistic        |
| Stochastic        | @rossbergFoodWebsExperts2006                 |                    |
| Transfer learning | @strydomFoodWebReconstruction2022            | latent trait space |
| Trait-based       | @caronAddressingEltonianShortfall2022        | mechanistic        |

: Lets make a table that gives an overview of the different topology generators that we will look at {#tbl-history}

### Datasets used

Here I think we need to span a variety of domains, at minimum aquatic and terrestrial but maybe there should be a 'scale' element as well *i.e.,* a regional and local network. I think there is going to be a 'turning point' where structural will take over from mechanistic in terms of performance. More specifically at local scales bioenergetic constraints (and co-occurrence) may play a bigger role in structuring a network whereas at the metaweb level then mechanistic may make more (since by default its about who can potentially interact and obviously not constrained by real-world scenarios) *sensu* @caronTrophicInteractionModels2023

## Results

How we want to compare and contrast. I think there won't be a 'winner' and thus we need to think of 'tests' that are going to measure performance in different situations/settings. With that in mind I think some valuable points to consider would be:

* Structural vs pairwise link predictions (graph vs node level)
  * % of links correctly retrieved
  * connectence
  * trophic level
  * generalism vs specialism
  * something related to false positives/negatives
* Data 'cost' (some methods might need a lot lot of supporting data vs something very light weight)
* I think it would be remiss to not also take into consideration computational cost
* something about the network output - I'm acknowledging my biases and saying that probabilistic (or *maybe* weighted) links are the way

## Conclusion

## References {.unnumbered}

::: {#refs}
:::