---
title: T is for Topology
author:
  - name: Tanya Strydom
    id: ts
    orcid: 0000-0001-6067-1349
    corresponding: true
    email: t.strydom@sheffield.ac.uk
    roles:
      - Words
      - Nonsense
      - Rascality
      - Visualisation (although absent)
    affiliation:
      - id: sheffield
        name: School of Biosciences, University of Sheffield
  - name: Andrew P. Beckerman
    id: apb
    orcid: 0000-0002-7859-8394
    corresponding: false
    roles: []
    affiliations:
      - ref: sheffield
funding: "The author(s) received no specific funding for this work. Well they did I just haven't done the "
keywords:
  - food web
  - network construction
abstract: |
  There are many reasons one might want to generate a network and there are many tools on the market that might make that possible. However not all tools are created equally and there is reason to assume that not all networks will suit most purposes. Here the aim is to compare and contrast the different topology generating tools that are on the market and see where they shine and where they fall flat. There probably isn't one model to rule them all but it doesn't mean that we shouldn't be critical when we think about the model we want to use.
plain-language-summary: |
  We want to know a bit more about the different network topology generators (prediction tools) and how they differ - *i.e.,*  their strengths and weaknesses
date: last-modified
bibliography: references.bib
citation:
  container-title: Some fancy journal
number-sections: true
jupyter: python3
---

## Introduction

* In order to construct a 'perfect' network *i.e.,* one which *perfectly* captures the dynamics for a specific community one needs to consider and account for many different moving parts (*e.g.,*). So when developing a model it makes sense that you prioritise the aspect of the prediction/construction task that has the most value for your research goal, acknowledging that a model might fall short in others. The thing is that with the growing suite of approaches to generating networks it is important that we don't lose sight of the core philosophy behind the model we use and to ensure that we are using the model best suited to what we want to be accomplishing.

* It is perhaps useful to start with asking why do we want/need models to generate networks. This can be broadly thought of to fall into two categories. Build networks because we want to build concepts vs build networks because we want specificity. Broadly this means that we either want to construct/predict a collection of interactions (generate networks) or a network of interactions (predict interactions).

Arguably the need for methods and tools for constructing interaction networks arises from two different (but still aligned) places of interest within the field of network ecology. On the one side sits the researcher who is interested in generating a set of ecologically plausible but not necessarily realised 'in the field' networks for the purpose of running further simulations (*e.g.,* extinction sim **TODO**) or understanding some higher-level process/concept (*e.g.,* energetics **TODO**). This researcher is contrasted by one that is interested in constructing real-world, location specific, interaction data in lieu of inventorying interactions based n observations made in the field [@strydomRoadmapPredictingSpecies2021]. Of course these two categories are not two distinct, mutually exclusive, groups but can rather be viewed as operating on a gradient ranging from a need for generality (*i.e.,* creating a network that, when taken in aggregate, the distribution of links (interactions) between species are ecologically plausible) to a need for specificity (local-level predictions between specific species).

* Brief history of the development of tools within the context of the two different fields? Sort of where the theory/body of work was based and how that has changed?

* A breakdown of wanting to generate a network; statement of need and core philosophies

* A breakdown of wanting to predict an interaction; statement of need ([@jordanoSamplingNetworksEcological2016; @jordanoChasingEcologicalInteractions2016; @poisotGlobalKnowledgeGaps2021]) and core philosophies (trait-matching, coexistence)

* Stands to reason then that we have developed methods that specialise in one or the other. Which comes at a cost of 'performance' in other aspects. Knowing how the different model families stack up to each other is thus valuable and that is kinda what we are trying to achieve.

@cohenStochasticTheoryCommunity1985 states that *"\[Their\] approach is more like gross anatomy than like physiology... that is, the gross anatomy is frozen, rather than in motion."*.

Interestingly @williamsSuccessItsLimits2008 also explicitly talk about *structural* food-web models in their introduction... so how I see it that means that there has always been this inherent acknowledgement that models are functioning at a specific 'network level'.

### Model families

Given the large number of models that have been developed it is perhaps more meaningful to group models into families with the idea that models from the same family will yield similar results because they play by similar rules. These rules referring to the underlying philosophy as to what structures either networks or the interactions within them (see @fig-concept panel A).

![Conceptual figure of the 'network prediction'. Panel A shows where the model families fall in the the context of being models that predict networks or models that predict interactions space. Panel B serves to highlight the characteristics one might like to 'test'/benchmark for a model based on it being either a network or interaction predicting model](images/concept.jpeg){#fig-concept}

**Null models:** The interactions between species occurs regardless of the identity of the species (*i.e.,* species have no agency) and links are randomly distributed throughout the network. There is however the assumption that a network will be constrained by the number of links. Type I [@fortunaHabitatLossStructure2006], where interactions happen proportionally to connectance and Type II [@bascompteNestedAssemblyPlantanimal2003], where interactions happen proportionally to the joint degree of the two species involved. These two models are equivalent to the Erdos-Renyi and Configuration models [@newmanNetworksIntroduction2010] (check that though).

**Neutral models:** Based on the theory that interactions occur as the result of the abundance of species (*i.e.,* the species still has no agency but its abundance does?). See @pomeranzInferringPredatorPrey2019

**Resource models:** In the context of network generating models this is perhaps the most well known family of models. Essentially these models can be viewed as being based on the idea of resource partitioning and that the number of links scale with species richness (maybe not directly that but these models are link constrained). That is there is some sort of hierarchical feeding based on how a 'resource' is partitioned. This includes the cascade model [@cohenCommunityFoodWebs1990], which much like the name suggests the cascade model rests on the idea that species feed on one another in a hierarchical manner. This rests on the assumption that the links within a network are variably distributed across the network; with the proportion of links decreasing as one moves up the trophic levels (*i.e.,* 'many' prey and 'few' predators). The niche model [@williamsSimpleRulesYield2000] introduces the idea that species interactions are based on the 'feeding niche' of a species. Broadly, all species are randomly assigned a 'feeding niche' and all species that fall in this niche can be consumed by that species. Finally, the nested hierarchy model[@cattinPhylogeneticConstraintsAdaptation2004] **TODO**.

**Energetic models:** Broadly this family of models is rooted in feeding theory and allocates the links between species based on energetics. This means that the model is focused on predicting not only the number of links in a network but also the arrangement of these links based on the diet breadth of a species. The diet breadth model [@beckermanForagingBiologyPredicts2006] as well as its allometrically scaled cousin the allometric diet breadth model (ADBM) [@petcheySizeForagingFood2008] determine links between species based on the energetic content, handling time, and density of species.

**Binary classifiers:** 

**Graph embedding:** This family of approaches has been extensively discussed in @strydomGraphEmbeddingTransfer2023 but can be broadly explained as an approach that estimates latent features from observed networks that can be used to predict interactions. @strydomFoodWebReconstruction2022 presents a specific use case that is based on transfer learning and the idea that interactions are evolutionarily conserved and that we can use known networks, and this evolutionary relationship to predict interactions for a given species pool.

**Trait hierarchy:** Here I envision models that present an *a priori* trait hierarchy that determines feeding links between species. That is, there is an element of 'expert knowledge' that also comes into play... Something like PFIM [@shawFrameworkReconstructingAncient2024] is what I imagine fitting in here...

**Expert knowledge:** (boots on the ground ecology)

### Model benchmarking

see @fig-concept panel B

Maybe start here with discussing the core mechanistic differences that models will work at --- some are really concerned about (and thus constrained by) structure, others are more mechanistic in nature *i.e.,* species *a* has the capacity to eat species *b* because traits (read gob size), and then you get @rohrModelingFoodWebs2010 and @strydomFoodWebReconstruction2022 that sit in the weird liminal latent space...

Here I will probably get on my (newly discovered) soapbox and wax lyrical about how in certain situations structure is enough (and that will probably be for some high-level things like thinking about energy flows etc., I can also see a world in which maybe you want to do some sort of robustness/extinction work - since then you're usually doing 'random' (within limits) extinctions) but there may be use cases where we are really interested in the node-level interactions *i.e.,* species identity is like a thing we need to care about and also be able to retrieve specific interactions at specific nodes correctly. What is the purpose of generating a network? Is it an element of a bigger question we are asking, *e.g.,* I want to generate a series of networks to do some extinction simulations/bioenergetic stuff OR are we looking for a 'final product' network that is relevant to a specific location? (this can still be broad in geographic scope).

At some point we are going to need to discuss the key differences and implications between predicting a metaweb (*sensu* @dunneNetworkStructureFood2006) and a network realisation. And here I can't help but think about @poisotSpeciesWhyEcological2015 (and probably other papers) that discuss how the local factors are going to play a role and even the same pair of species may interact differently in different points in the landscape.

## Data & Methods {#sec-data-methods}

### Selecting modles

Here we only look at families of models that are explicitly developed to construct *de novo* networks, be this in the form of either artificial or synthetic networks.

### Topology Generators

> @gravelInferringFoodWeb2013 also poses an interesting cross-over between the adbm and niche model.

### Interaction Predictors

**Log-ratio** [@rohrModelingFoodWebs2010]: Interestingly often used in paleo settings (at least that's what it currently looks like in my mind... [*e.g.,* @yeakelCollapseEcologicalNetwork2014, @piresMegafaunalExtinctionsHuman2020])

**Matching** [@rossbergFoodWebsExperts2006]: This one is more of a dynamic model (so BEF) and maybe beyond the scope of this work. I think there is value on only focusing on the 'static' models at this point (probably have said this before elsewhere but yeah)

**Trait-based** [@caronAddressingEltonianShortfall2022]:

I know tables are awful but in this case they may make more sense. Also I don't think I'm at the point where I can say that the table is complete/comprehensive but it getting there Not sure about putting in some papers that have used the model - totes happy to drop those I think...

| Model               | Core Mechanism | Predicts     | Specificity      | Interaction   | Data-driven |
|------------|------------|------------|------------|------------|------------|
| random              | random         | networks     | species agnostic | binary        | no          |
| cascade             | structural     | networks     | species agnostic | binary        | no          |
| niche               | structural     | networks     | species agnostic | binary        | no          |
| nested hierarchical | structural     | networks     | species agnostic | binary        | no          |
| ADBM                | mechanistic    | interactions | energetics       | quantitative  |             |
| log-ratio           |                | interactions |                  |               |             |
| PFIM                | mechanistic    | interactions | trait based      |               |             |
| graph embedding     | embedding      | interactions | evolutionary     | probabilistic | yes         |
| trait model         | mechanistic    | interactions | trait based      |               | yes         |
| matching            |                |              |                  |               |             |

: Lets make a table that gives an overview of the different topology generators that we will look at. Here I take 'data-driven' to refer to the need for 'real world' data. This can probably be approached in a different way though maybe? {#tbl-history}

> Might be nice to have a little appendix/supp mat that breaks down the models in detail so that they are all in one place so that someone (grad student being told they need to build networks) some day can go and educate themselves with slightly lower effort. This will also be useful for me should I end up having to do some actual coding - think of this as step one in the pseudo code process.

### Datasets used

Here I think we need to span a variety of domains, at minimum aquatic and terrestrial but maybe there should be a 'scale' element as well *i.e.,* a regional and local network. I think there is going to be a 'turning point' where structural will take over from mechanistic in terms of performance. More specifically at local scales bioenergetic constraints (and co-occurrence) may play a bigger role in structuring a network whereas at the metaweb level then mechanistic may make more (since by default its about who can potentially interact and obviously not constrained by real-world scenarios) *sensu* @caronTrophicInteractionModels2023. Although having said that I feel that contradicts the idea of backbones (*sensu* Bramon Mora (sp?) et al & Stouffer et al) But that might be where we get the idea of core *structure* vs something like linkage density. So core things like trophic level/chain length will be conserved but connectance might not (I think I understand what I'm trying to say here)

I think we should also use the Dunne (I think) Cambrian (also think) network (I was correct and its this one @dunneCompilationNetworkAnalyses2008). Because 1) it gives the paleo-centric methods their moment in the sun and 2) I think it also brings up the interesting question of can we use modern structure to predict past ones? Here one might expect a more mechanistic approach to shine.

Draw the other datasets from `Mangal` because they will be nicely formatted and essentially at point and shoot level

### Comparing different models

For now the (still essentially pending) workflow/associated code can be found at the following repository [BecksLab/topology_generators](https://github.com/BecksLab/topology_generators)

1.  Shortlist/finalise the different topo generators
2.  collate/translate into `Julia`
    -   *e.g.,* some models wil be in SpeciesInteractionNetworks.jl (new EcoNet); I know (parts of) the transfer learning stuff is and the niche model
    -   others will need to be coded out (the more simpler models should be easier)
    -   can also consider `R` but then it becomes a case of porting things left and right depending on how we decide to do the post analyses
3.  Curate networks for the different datasets/scenarios we select - I feel like there might be some scenarios that we can't do all models for all datasets but maybe I'm being a pessimist.
    -   Need to also think about where one might find the additional data for some of the models...
        -   Body size: @herbersteinAnimalTraitsCuratedAnimal2022 - Although maybe Andrew has strong thotsTM RE the one true body size database to rule them all...
        -   Other trait sources: @wilmanEltonTraitsSpecieslevelForaging2014 and @jonesPanTHERIASpecieslevelDatabase2009
        -   This is where we'll get the paleo traits from if I'm correct @bambachAutecologyFillingEcospace2007
        -   Phylogeny stuff: @uphamInferringMammalTree2019 (what we used for TL but its only mammals...) but I'm sure there will be others
    -   Also limitation of scope... *e.g.,* do we even dare to think about including plants/basal producers (see *e.g.,* @valdovinosBioenergeticFrameworkAboveground2023)
    -   Taxonomic harmonisation - something to think about and check
4.  compare model performance based on the ideas currently listed in the results section.
5.  Make a pretty picture that summarises things - maybe overlapping Venn circles that showcase which models do well in the different spheres/aspects of life

## Results

How we want to compare and contrast. I think there won't be a 'winner' and thus we need to think of 'tests' that are going to measure performance in different situations/settings. With that in mind I think some valuable points to consider would be:

-   Structural vs pairwise link predictions (graph vs node level)
    -   \% of links correctly retrieved
    -   connectance
    -   trophic level
    -   generalism vs specialism
    -   something related to false positives/negatives
    -   intervality
-   Data 'cost' (some methods might need a lot lot of supporting data vs something very light weight)
-   I think it would be remiss to not also take into consideration computational cost
-   something about the network output - I'm acknowledging my biases and saying that probabilistic (or *maybe* weighted) links are the way

@cohenStochasticTheoryCommunity1985 actually tells us that the cascade model only really works for communities that range from 3-33 species... and @williamsSuccessItsLimits2008 also highlights how structural models really only work for small communities

> maybe we can put these into broader categories - if we do start doing the venn overlap thing. *E.g.,* local scale predictions, regional scale predictions, pairwise interactions, structural (energetics), computationally cheap, low cost data

### Quantitative stuff

{{< embed notebooks/model_quantitative.qmd#fig-topology >}}

This is actually an awful way to try and summarise the data but rolling with it for now...

## Discussion

I think a big take home will (hopefully) be how different approaches do better in different situations and so you as an end user need to take this into consideration and pick accordingly. I think @petcheyFitEfficiencyBiology2011 might have (and share) some thoughts on this (thanks Andrew). I feel like I need to look at @berlowGoldilocksFactorFood2008 but maybe not exactly in this context but vaguely adjacent.

An interesting thing to also think about (and arguably it will be addressed based on some of the other thoughts and ideas) is data dependant and data independent 'parametrisation' of the models...

I probably think about this point too much but a point of discussion that I think will be interesting to bring up the idea that if a model is missing a specific pairwise link but doing well at the structural level then when does it matter? I think this is covered with the whole node vs graph level performance but I kind of just want to bring it up here again because also one of those things that I think about a bit too much probably...

> Thinking very long term here and maybe a bit beyond the scope but also thinking about a multi- model approach? So in other words using one model to build an initial network but maybe a second one to constrain it a bit better. I blame this thought on the over-connected PFIM food webs...

## References {.unnumbered}

::: {#refs}
:::